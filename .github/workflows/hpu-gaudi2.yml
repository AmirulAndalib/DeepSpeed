name: hpu-gaudi2

on:
  workflow_dispatch:
  schedule:
    - cron: "0 0 * * *"
  pull_request:
    paths:
      - ".github/workflows/hpu-gaudi2.yml"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  issues: write

jobs:
  unit-tests:
    # The type of runner that the job will run on
    runs-on: [self-hosted, intel, gaudi2]
    container:
      image: vault.habana.ai/gaudi-docker/1.14.0/ubuntu22.04/habanalabs/pytorch-installer-2.1.1:latest
      ports:
        - 80
      options: --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v3

      - name: Check container state
        run: |
          ldd --version
          hl-smi
          python -c "import torch; print('torch:', torch.__version__, torch)"
          python -c "import torch; print('CUDA available:', torch.cuda.is_available())"

      - name: Install transformers
        run: |
          git clone https://github.com/huggingface/transformers
          cd transformers
          git rev-parse --short HEAD
          pip install .

      - name: Install deepspeed
        run: |
          pip install .[dev,autotuning]
          ds_report

      - name: Python environment
        run: |
          pip list

      - name: Unit tests
        run: |
          unset TORCH_CUDA_ARCH_LIST # only jit compile for current arch
          set PT_HPU_LAZY_MODE=0
          cd tests
          pytest -n 4 --verbose -vs unit/accelerator/test_accelerator.py unit/autotuning/test_autotuning.py unit/compression/test_compression.py unit/comm/test_dist.py unit/elasticity/test_elastic.py
